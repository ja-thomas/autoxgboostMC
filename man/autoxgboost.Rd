% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autoxgboost.R
\name{autoxgboost}
\alias{autoxgboost}
\title{Fit and optimize a xgboost model.}
\usage{
autoxgboost(task, measure = NULL, control = NULL, iterations = 160L,
  time.budget = 3600L, par.set = NULL, max.nrounds = 10^6,
  early.stopping.rounds = 10L, early.stopping.fraction = 4/5,
  build.final.model = TRUE, design.size = 15L,
  impact.encoding.boundary = 10L, mbo.learner = NULL, nthread = NULL,
  tune.threshold = TRUE)
}
\arguments{
\item{task}{[\code{\link[mlr]{Task}}]\cr
The task.}

\item{measure}{[\code{\link[mlr]{Measure}}]\cr
Performance measure. If \code{NULL} \code{\link[mlr]{getDefaultMeasure}} is used.}

\item{control}{[\code{\link[mlrMBO]{MBOControl}}]\cr
Control object for optimizer.
If not specified, the default \code{\link[mlrMBO]{makeMBOControl}}] object will be used with
\code{iterations} maximum iterations and a maximum runtime of \code{time.budget} seconds.}

\item{iterations}{[\code{integer(1L}]\cr
Number of MBO iterations to do. Will be ignored if custom \code{control} is used.
Default is \code{160}.}

\item{time.budget}{[\code{integer(1L}]\cr
Time that can be used for tuning (in seconds). Will be ignored if custom \code{control} is used.
Default is \code{3600}, i.e., one hour.}

\item{par.set}{[\code{\link[ParamHelpers]{ParamSet}}]\cr
Parameter set to tune over. Default is \code{\link{autoxgbparset}}.}

\item{max.nrounds}{[\code{integer(1)}]\cr
Maximum number of allowed boosting iterations. Default is \code{10^6}.}

\item{early.stopping.rounds}{[\code{integer(1L}]\cr
After how many iterations without an improvement in the boosting OOB error should be stopped?
Default is \code{10}.}

\item{early.stopping.fraction}{[\code{numeric(1)}]\cr
What fraction of the data should be used for early stopping (i.e. as a validation set).
Default is \code{4/5}.}

\item{build.final.model}{[\code{logical(1)}]\cr
Should the model with the best found configuration be refitted on the complete dataset?
Default is \code{FALSE}.}

\item{design.size}{[\code{integer(1)}]\cr
Size of the initial design. Default is \code{15L}.}

\item{impact.encoding.boundary}{[\code{integer(1)}]\cr
Defines the threshold on how factor variables are handled. Factors with more levels than the \code{"impact.encoding.boundary"} get impact encoded while factor variables with less or equal levels than the \code{"impact.encoding.boundary"} get dummy encoded.
For \code{impact.encoding.boundary = 0L}, all factor variables get impact encoded while for \code{impact.encoding.boundary = .Machine$integer.max}, all of them get dummy encoded.
Default is \code{10}.}

\item{mbo.learner}{[\code{\link[mlr]{Learner}}]\cr
Regression learner from mlr, which is used as a surrogate to model our fitness function.
If \code{NULL} (default), the default learner is determined as described here: \link[mlrMBO]{mbo_default_learner}.}

\item{nthread}{[integer(1)]\cr
Number of cores to use.
If \code{NULL} (default), xgboost will determine internally how many cores to use.}

\item{tune.threshold}{[logical(1)]\cr
Should thresholds be tuned? This has only an effect for classification, see \code{\link[mlr]{tuneThreshold}}.
Default is \code{TRUE}.}
}
\value{
\code{\link{AutoxgbResult}}
}
\description{
An xgboost model is optimized based on a measure (see [\code{\link[mlr]{Measure}}]).
The bounds of the parameter in which the model is optimized, are defined by \code{\link{autoxgbparset}}.
For the optimization itself bayesian optimization with \pkg{mlrMBO} is used.
Without any specification of the control object, the optimizer runs for for 80 iterations or 1 hour, whatever happens first.
Both the parameter set and the control object can be set by the user.
}
\examples{
\donttest{
iris.task = makeClassifTask(data = iris, target = "Species")
ctrl = makeMBOControl()
ctrl = setMBOControlTermination(ctrl, iters = 1L) #Speed up Tuning by only doing 1 iteration
res = autoxgboost(iris.task, control = ctrl, tune.threshold = FALSE)
res
}
}
